% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/entropy_utilities.R
\name{entropy_nd}
\alias{entropy_nd}
\title{Multivariate Differential Entropy Estimation with the Kozachenko–Leonenko Method}
\usage{
entropy_nd(data, k = 10, normalize = c("none", "divide", "sqrt"))
}
\arguments{
\item{data}{A data frame or a matrix of size \eqn{n \times d},
where \eqn{n} is the number of observations and \eqn{d} the dimensionality.}

\item{k}{Number of neighbors to consider in the estimator.
Must be a positive integer, typically \eqn{k \geq 2}.
Default: 10.}

\item{normalize}{Normalization mode of the returned entropy.
Can be:
\itemize{
\item \code{"none"}: raw entropy in bits;
\item \code{"divide"}: per-dimension entropy (bits per dimension);
\item \code{"sqrt"}: square-root scaling of entropy (non-standard heuristic).
}
Default: \code{"none"}.}
}
\value{
A single numeric value representing the estimated entropy
(possibly normalized) in bits.
}
\description{
This function computes the differential entropy of a multivariate dataset
using the k-nearest neighbor estimator (Kozachenko–Leonenko).
}
\details{
The Kozachenko–Leonenko estimator is based on the distance to the
\eqn{k}-th nearest neighbor for each point, and the volume of the unit ball in \eqn{R^d}.
The entropy is computed in nats and then converted to bits.
}
\examples{
set.seed(123)
data <- matrix(rnorm(100 * 3), ncol = 3)
entropy_nd(data, k = 5, normalize = "none")
entropy_nd(data, k = 5, normalize = "divide")

}
\references{
Kozachenko, L. F., & Leonenko, N. N. (1987).
Sample estimate of the entropy of a random vector.
\emph{Problemy Peredachi Informatsii}, 23(2), 9–16.
}
